{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook extract_hotel_feature.ipynb to python\n",
      "[NbConvertApp] Writing 6984 bytes to extract_hotel_feature.py\n",
      "[NbConvertApp] Converting notebook extract_basic_room_feature.ipynb to python\n",
      "[NbConvertApp] Writing 8108 bytes to extract_basic_room_feature.py\n",
      "[NbConvertApp] Converting notebook extract_hotel_room_feature.ipynb to python\n",
      "[NbConvertApp] Writing 3217 bytes to extract_hotel_room_feature.py\n",
      "[NbConvertApp] Converting notebook extract_room_feature.ipynb to python\n",
      "[NbConvertApp] Writing 1483 bytes to extract_room_feature.py\n",
      "[NbConvertApp] Converting notebook extract_uid_feture.ipynb to python\n",
      "[NbConvertApp] Writing 9011 bytes to extract_uid_feture.py\n",
      "[NbConvertApp] Converting notebook extract_order_feature.ipynb to python\n",
      "[NbConvertApp] Writing 25998 bytes to extract_order_feature.py\n",
      "[NbConvertApp] Converting notebook combine_sample.ipynb to python\n",
      "[NbConvertApp] Writing 1710 bytes to combine_sample.py\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert --to python extract_hotel_feature.ipynb\n",
    "!jupyter nbconvert --to python extract_basic_room_feature.ipynb\n",
    "!jupyter nbconvert --to python extract_hotel_room_feature.ipynb\n",
    "!jupyter nbconvert --to python extract_room_feature.ipynb\n",
    "!jupyter nbconvert --to python extract_uid_feture.ipynb\n",
    "!jupyter nbconvert --to python extract_order_feature.ipynb\n",
    "!jupyter nbconvert --to python combine_sample.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "08/06/2017 08:48:38 PM INFO: Reading notebook mult_run.ipynb\n",
      "08/06/2017 08:48:39 PM INFO: Running cell:\n",
      "import time\n",
      "from datetime import datetime\n",
      "from os import mkdir\n",
      "from os.path import exists, join\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import scipy as sp\n",
      "\n",
      "import psutil\n",
      "\n",
      "08/06/2017 08:48:40 PM INFO: Cell returned\n",
      "08/06/2017 08:48:40 PM INFO: Running cell:\n",
      "if not exists('logs'):\n",
      "    mkdir(log_dir)\n",
      "\n",
      "08/06/2017 08:48:40 PM INFO: Cell returned\n",
      "08/06/2017 08:48:40 PM INFO: Running cell:\n",
      "now = datetime.now()\n",
      "log_dir = join('logs', now.strftime('%d-%H'))\n",
      "if not exists(log_dir):\n",
      "    mkdir(log_dir)\n",
      "log_dir\n",
      "\n",
      "08/06/2017 08:48:40 PM INFO: Cell returned\n",
      "08/06/2017 08:48:40 PM INFO: Running cell:\n",
      "worker_dirs = ['train'] + ['test']\n",
      "\n",
      "08/06/2017 08:48:40 PM INFO: Cell returned\n",
      "08/06/2017 08:48:40 PM INFO: Running cell:\n",
      "feture_extract_path = ['extract_hotel_feature', 'extract_basic_room_feature',\n",
      "                      'extract_hotel_room_feature', 'extract_room_feature',\n",
      "                      'extract_uid_feture']\n",
      "\n",
      "08/06/2017 08:48:40 PM INFO: Cell returned\n",
      "08/06/2017 08:48:40 PM INFO: Running cell:\n",
      "# feture_extract_path = ['extract_order_feature']\n",
      "\n",
      "08/06/2017 08:48:40 PM INFO: Cell returned\n",
      "08/06/2017 08:48:40 PM INFO: Running cell:\n",
      "# feture_combine_path = ['combine_sample']\n",
      "feture_combine_path = ['extract_order_feature']\n",
      "\n",
      "08/06/2017 08:48:40 PM INFO: Cell returned\n",
      "08/06/2017 08:48:40 PM INFO: Running cell:\n",
      "max_run = 4\n",
      "\n",
      "08/06/2017 08:48:40 PM INFO: Cell returned\n",
      "08/06/2017 08:48:40 PM INFO: Running cell:\n",
      "def get_args(feture_extract_path, worker_dirs):\n",
      "    for p in feture_extract_path:\n",
      "        for d in worker_dirs:\n",
      "            yield [p, d]\n",
      "\n",
      "08/06/2017 08:48:40 PM INFO: Cell returned\n",
      "08/06/2017 08:48:40 PM INFO: Running cell:\n",
      "def check(p):\n",
      "    pm = p.memory_info()\n",
      "    is_zero = all(map(lambda x:getattr(pm, x)==0, ['rss','vms', 'shared', 'text', 'lib', 'data', 'dirty']))\n",
      "    return is_zero\n",
      "\n",
      "08/06/2017 08:48:40 PM INFO: Cell returned\n",
      "08/06/2017 08:48:40 PM INFO: Running cell:\n",
      "def is_wait(pool, max_pool=4):\n",
      "    if len(pool) < max_pool:\n",
      "        print(datetime.now(), 'pool size', len(pool))\n",
      "        return False\n",
      "    else:\n",
      "        time.sleep(10)\n",
      "        not_run_p = []\n",
      "        for i, p in enumerate(pool[:]):\n",
      "            not_run = check(p)\n",
      "            print(datetime.now(), 'process', i, 'not runing' if not_run else 'run')\n",
      "            if not_run:\n",
      "                not_run_p.append(i)\n",
      "        not_run_p.sort(reverse=True)\n",
      "        for i in not_run_p:\n",
      "            p = pool.pop(i)\n",
      "            p.kill()\n",
      "        return True\n",
      "\n",
      "08/06/2017 08:48:40 PM INFO: Cell returned\n",
      "08/06/2017 08:48:40 PM INFO: Running cell:\n",
      "def run_args(extract_args, max_pool=4):\n",
      "    pool = []\n",
      "    for p, n in extract_args:\n",
      "        print(datetime.now(), 'begin', p, n)\n",
      "        fn = join(log_dir, '{}_{}.txt'.format(p, n))\n",
      "        pool.append(psutil.Popen(['python', p+'.py', n], stdout=open(fn, 'w')))\n",
      "        while is_wait(pool, max_pool):\n",
      "            print(datetime.now(), 'run in ', p, n)\n",
      "    while is_wait(pool, 1):\n",
      "        print('wait for pool over')\n",
      "\n",
      "08/06/2017 08:48:40 PM INFO: Cell returned\n",
      "08/06/2017 08:48:40 PM INFO: Running cell:\n",
      "# for p, n in extract_args:\n",
      "#     print(datetime.now(), 'begin', p, n)\n",
      "#     fn = join(log_dir, '{}_{}.txt'.format(p, n))\n",
      "#     pool.append(psutil.Popen(['python', p+'.py', n], stdout=open(fn, 'w')))\n",
      "#     while is_wait(pool):\n",
      "#         print(datetime.now(), 'run in ', p, n)\n",
      "\n",
      "08/06/2017 08:48:40 PM INFO: Cell returned\n",
      "08/06/2017 08:48:40 PM INFO: Running cell:\n",
      "extract_args = get_args(feture_extract_path, worker_dirs)\n",
      "combine_args = get_args(feture_combine_path, worker_dirs)\n",
      "\n",
      "08/06/2017 08:48:40 PM INFO: Cell returned\n",
      "08/06/2017 08:48:40 PM INFO: Running cell:\n",
      "run_args(extract_args, max_run)\n",
      "\n",
      "extract_hotel_feature.py:267: UserWarning: uid_shape not equal [uid ,hotelid_lastord, hotelid]\n",
      "  warn('uid_shape not equal [uid ,hotelid_lastord, hotelid]')\n",
      "08/06/2017 08:49:20 PM INFO: Cell returned\n",
      "08/06/2017 08:49:20 PM INFO: Running cell:\n",
      "run_args(combine_args, max_run)\n",
      "\n",
      "extract_order_feature.py:44: FutureWarning: currently extract(expand=None) means expand=False (return Index/Series/DataFrame) but in a future version of pandas this will be changed to expand=True (return DataFrame)\n",
      "  nidf = idf.loc[idf.index.str.extract('^(orderid)').notnull()]\n",
      "extract_order_feature.py:44: FutureWarning: currently extract(expand=None) means expand=False (return Index/Series/DataFrame) but in a future version of pandas this will be changed to expand=True (return DataFrame)\n",
      "  nidf = idf.loc[idf.index.str.extract('^(orderid)').notnull()]\n",
      "/home/zhanglun/notebook/room/code/utils.py:30: UserWarning: column  order_weekday is  may be error when meet percent max:0\n",
      "  warn('column  {} is  may be error when meet percent max:{}'.format(c, c_max))\n",
      "/home/zhanglun/notebook/room/code/utils.py:30: UserWarning: column  price_tail1 is  may be error when meet percent max:0.0\n",
      "  warn('column  {} is  may be error when meet percent max:{}'.format(c, c_max))\n",
      "/home/zhanglun/notebook/room/code/utils.py:30: UserWarning: column  price_tail1 is  may be error when meet percent max:0.0\n",
      "  warn('column  {} is  may be error when meet percent max:{}'.format(c, c_max))\n",
      "/home/zhanglun/.pyenv/versions/anaconda3-4.1.0/lib/python3.5/site-packages/pandas/core/frame.py:2746: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  **kwargs)\n",
      "/home/zhanglun/.pyenv/versions/anaconda3-4.1.0/lib/python3.5/site-packages/pandas/core/frame.py:2746: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  **kwargs)\n",
      "08/06/2017 08:58:22 PM INFO: Cell returned\n",
      "08/06/2017 08:58:22 PM INFO: Shutdown kernel\n"
     ]
    }
   ],
   "source": [
    "!runipy mult_run.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-08-06 20:58:25.549436 begin combine ../dataset/train/all_feature.pkl\r\n",
      "b''\r\n",
      "2017-08-06 20:58:25.554374 save to ../dataset/train/all_feature.pkl\r\n"
     ]
    }
   ],
   "source": [
    "!python combine_sample.py train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-08-06 20:58:26.584692 begin combine ../dataset/test/all_feature.pkl\r\n",
      "b''\r\n",
      "2017-08-06 20:58:26.589373 save to ../dataset/test/all_feature.pkl\r\n"
     ]
    }
   ],
   "source": [
    "!python combine_sample.py test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "08/06/2017 08:58:27 PM INFO: Reading notebook split_train_sample.ipynb\n",
      "08/06/2017 08:58:28 PM INFO: Running cell:\n",
      "import pickle\n",
      "from datetime import datetime\n",
      "from os import mkdir\n",
      "from os.path import join, exists\n",
      "\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import scipy as sp\n",
      "\n",
      "# import matplotlib.pyplot as plt\n",
      "\n",
      "from sklearn import model_selection\n",
      "from sklearn.model_selection import KFold\n",
      "from sklearn.model_selection import train_test_split\n",
      "\n",
      "import lightgbm as lgb\n",
      "from lightgbm import LGBMClassifier\n",
      "\n",
      "from sklearn import model_selection\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from sklearn.naive_bayes import GaussianNB \n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from mlxtend.classifier import StackingClassifier\n",
      "from mlxtend.classifier import StackingCVClassifier\n",
      "\n",
      "from sklearn import metrics\n",
      "\n",
      "08/06/2017 08:58:29 PM INFO: Cell returned\n",
      "08/06/2017 08:58:29 PM INFO: Running cell:\n",
      "luck=123456\n",
      "\n",
      "08/06/2017 08:58:29 PM INFO: Cell returned\n",
      "08/06/2017 08:58:29 PM INFO: Running cell:\n",
      "train_features = pd.read_pickle('../dataset/train/all_feature.pkl')\n",
      "train_features.shape\n",
      "\n",
      "08/06/2017 08:58:30 PM INFO: Cell returned\n",
      "08/06/2017 08:58:30 PM INFO: Running cell:\n",
      "user_id = train_features.uid.drop_duplicates().reset_index(drop=True).reset_index().set_index('uid')\n",
      "\n",
      "08/06/2017 08:58:31 PM INFO: Cell returned\n",
      "08/06/2017 08:58:31 PM INFO: Running cell:\n",
      "train_user, test_user = train_test_split(user_id, random_state=luck, test_size=0.2)\n",
      "\n",
      "08/06/2017 08:58:31 PM INFO: Cell returned\n",
      "08/06/2017 08:58:31 PM INFO: Running cell:\n",
      "train_features = train_features.join(train_user, on='uid')\n",
      "\n",
      "08/06/2017 08:58:32 PM INFO: Cell returned\n",
      "08/06/2017 08:58:32 PM INFO: Running cell:\n",
      "train_f = train_features.loc[train_features['index'].notnull()]\n",
      "\n",
      "08/06/2017 08:58:34 PM INFO: Cell returned\n",
      "08/06/2017 08:58:34 PM INFO: Running cell:\n",
      "train_f.drop('index', axis=1, inplace=True)\n",
      "\n",
      "08/06/2017 08:58:34 PM INFO: Cell returned\n",
      "08/06/2017 08:58:34 PM INFO: Running cell:\n",
      "train_f.to_pickle('../dataset/train/train_all_feature.pkl')\n",
      "\n",
      "08/06/2017 08:58:36 PM INFO: Cell returned\n",
      "08/06/2017 08:58:36 PM INFO: Running cell:\n",
      "test_f = train_features.loc[train_features['index'].isnull()]\n",
      "\n",
      "08/06/2017 08:58:37 PM INFO: Cell returned\n",
      "08/06/2017 08:58:37 PM INFO: Running cell:\n",
      "test_f.drop('index', axis=1, inplace=True)\n",
      "\n",
      "08/06/2017 08:58:37 PM INFO: Cell returned\n",
      "08/06/2017 08:58:37 PM INFO: Running cell:\n",
      "test_f.to_pickle('../dataset/train/test_all_feature.pkl')\n",
      "\n",
      "08/06/2017 08:58:37 PM INFO: Cell returned\n",
      "08/06/2017 08:58:37 PM INFO: Shutdown kernel\n"
     ]
    }
   ],
   "source": [
    "!runipy split_train_sample.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "08/06/2017 08:58:39 PM INFO: Reading notebook lgb_train.ipynb\n",
      "08/06/2017 08:58:39 PM INFO: Running cell:\n",
      "import pickle\n",
      "import gc\n",
      "from datetime import datetime\n",
      "from os import mkdir\n",
      "from os.path import join, exists\n",
      "\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import scipy as sp\n",
      "\n",
      "# import matplotlib.pyplot as plt\n",
      "\n",
      "from sklearn import model_selection\n",
      "from sklearn.model_selection import KFold\n",
      "from sklearn.model_selection import train_test_split\n",
      "\n",
      "import lightgbm as lgb\n",
      "from lightgbm import LGBMClassifier\n",
      "\n",
      "from sklearn import model_selection\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from sklearn.naive_bayes import GaussianNB \n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from mlxtend.classifier import StackingClassifier\n",
      "from mlxtend.classifier import StackingCVClassifier\n",
      "\n",
      "from sklearn import metrics\n",
      "\n",
      "08/06/2017 08:58:40 PM INFO: Cell returned\n",
      "08/06/2017 08:58:40 PM INFO: Running cell:\n",
      "train_f = pd.read_pickle('../dataset/train/train_all_feature.pkl')\n",
      "train_f.shape\n",
      "\n",
      "08/06/2017 08:58:41 PM INFO: Cell returned\n",
      "08/06/2017 08:58:41 PM INFO: Running cell:\n",
      "test_f = pd.read_pickle('../dataset/train/test_all_feature.pkl')\n",
      "test_f.shape\n",
      "\n",
      "08/06/2017 08:58:41 PM INFO: Cell returned\n",
      "08/06/2017 08:58:41 PM INFO: Running cell:\n",
      "model_path = '../dataset/lightgbmmodel.pkl'\n",
      "\n",
      "08/06/2017 08:58:41 PM INFO: Cell returned\n",
      "08/06/2017 08:58:41 PM INFO: Running cell:\n",
      "luck = 123456\n",
      "\n",
      "08/06/2017 08:58:41 PM INFO: Cell returned\n",
      "08/06/2017 08:58:41 PM INFO: Running cell:\n",
      "model = LGBMClassifier(nthread=4, silent=False, objective= 'binary', seed=luck)\n",
      "\n",
      "08/06/2017 08:58:41 PM INFO: Cell returned\n",
      "08/06/2017 08:58:41 PM INFO: Running cell:\n",
      "parameters = {\n",
      "#     'gamma': [0.05],\n",
      "    #     'n_estimators': [800],\n",
      "    'max_depth': [2, 6, 12],\n",
      "#     'max_depth': [12],\n",
      "#     'num_leaves': range(30, 50, 4), \n",
      "\n",
      "#     'learning_rate': [0.15, 0.3],\n",
      "#         'subsample': [0.9],\n",
      "#         'colsample_bytree': [0.9],\n",
      "    #     'reg_alpha': [0, 1, 5],\n",
      "    #     'reg_lambda': [0, 1, 4],\n",
      "}\n",
      "\n",
      "08/06/2017 08:58:41 PM INFO: Cell returned\n",
      "08/06/2017 08:58:41 PM INFO: Running cell:\n",
      "use_columns = [\n",
      "    x for x in train_f.columns\n",
      "    if x not in [\n",
      "        'orderid', 'uid', 'hotelid', 'basicroomid', 'hotel_roomid', 'roomid',\n",
      "        'orderlabel', 'index'\n",
      "    ]\n",
      "]\n",
      "\n",
      "08/06/2017 08:58:41 PM INFO: Cell returned\n",
      "08/06/2017 08:58:41 PM INFO: Running cell:\n",
      "cv = model_selection.ShuffleSplit(n_splits=1, test_size=0.2, random_state=luck)\n",
      "clf = model_selection.GridSearchCV(model, parameters, cv=cv, n_jobs=-1, verbose=True)\n",
      "\n",
      "08/06/2017 08:58:41 PM INFO: Cell returned\n",
      "08/06/2017 08:58:41 PM INFO: Running cell:\n",
      "clf_feture = train_f.sample(50000, random_state=luck)\n",
      "\n",
      "08/06/2017 08:58:41 PM INFO: Cell returned\n",
      "08/06/2017 08:58:41 PM INFO: Running cell:\n",
      "%time clf.fit(clf_feture[use_columns], clf_feture['orderlabel'])\n",
      "\n",
      "[LightGBM] [Warning] Unknown parameter categorical_column=\n",
      "[LightGBM] [Warning] Unknown parameter categorical_column=\n",
      "[LightGBM] [Warning] Unknown parameter categorical_column=\n",
      "[LightGBM] [Info] Number of positive: 1162, number of negative: 38838\n",
      "[LightGBM] [Info] Total Bins 51319\n",
      "[LightGBM] [Info] Number of data: 40000, number of used features: 430\n",
      "[LightGBM] [Info] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Trained a tree with leaves=4 and max_depth=2\n",
      "[LightGBM] [Info] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Trained a tree with leaves=4 and max_depth=2\n",
      "[LightGBM] [Info] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Trained a tree with leaves=4 and max_depth=2\n",
      "[LightGBM] [Info] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Trained a tree with leaves=4 and max_depth=2\n",
      "[LightGBM] [Info] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Trained a tree with leaves=4 and max_depth=2\n",
      "[LightGBM] [Info] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Trained a tree with leaves=4 and max_depth=2\n",
      "[LightGBM] [Info] Number of positive: 1162, number of negative: 38838\n",
      "[LightGBM] [Info] Total Bins 51319\n",
      "[LightGBM] [Info] Number of data: 40000, number of used features: 430\n",
      "[LightGBM] [Info] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Trained a tree with leaves=4 and max_depth=2\n",
      "[LightGBM] [Info] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Trained a tree with leaves=4 and max_depth=2\n",
      "[LightGBM] [Info] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Trained a tree with leaves=4 and max_depth=2\n",
      "[LightGBM] [Info] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Trained a tree with leaves=4 and max_depth=2\n",
      "[LightGBM] [Info] Number of positive: 1162, number of negative: 38838\n",
      "[LightGBM] [Info] Total Bins 51319\n",
      "[LightGBM] [Info] Trained a tree with leaves=31 and max_depth=6\n",
      "[LightGBM] [Info] Number of data: 40000, number of used features: 430\n",
      "[LightGBM] [Info] Trained a tree with leaves=31 and max_depth=9\n",
      "[LightGBM] [Info] Trained a tree with leaves=31 and max_depth=6\n",
      "[LightGBM] [Info] Trained a tree with leaves=31 and max_depth=6\n",
      "[LightGBM] [Info] Trained a tree with leaves=31 and max_depth=11\n",
      "[LightGBM] [Info] Trained a tree with leaves=31 and max_depth=6\n",
      "[LightGBM] [Info] Trained a tree with leaves=31 and max_depth=12\n",
      "[LightGBM] [Info] Trained a tree with leaves=31 and max_depth=6\n",
      "[LightGBM] [Info] Trained a tree with leaves=31 and max_depth=8\n",
      "[LightGBM] [Info] Trained a tree with leaves=31 and max_depth=6\n",
      "[LightGBM] [Info] Trained a tree with leaves=31 and max_depth=9\n",
      "[LightGBM] [Info] Trained a tree with leaves=31 and max_depth=6\n",
      "[LightGBM] [Info] Trained a tree with leaves=31 and max_depth=11\n",
      "[LightGBM] [Info] Trained a tree with leaves=31 and max_depth=6\n",
      "[LightGBM] [Info] Trained a tree with leaves=31 and max_depth=10\n",
      "[LightGBM] [Info] Trained a tree with leaves=31 and max_depth=6\n",
      "[LightGBM] [Info] Trained a tree with leaves=31 and max_depth=8\n",
      "[LightGBM] [Info] Trained a tree with leaves=31 and max_depth=6\n",
      "[LightGBM] [Info] Trained a tree with leaves=31 and max_depth=9\n",
      "[LightGBM] [Info] Trained a tree with leaves=31 and max_depth=8\n",
      "[LightGBM] [Warning] Unknown parameter categorical_column=\n",
      "[LightGBM] [Info] Number of positive: 1433, number of negative: 48567\n",
      "[LightGBM] [Info] Total Bins 51573\n",
      "[LightGBM] [Info] Number of data: 50000, number of used features: 430\n",
      "[LightGBM] [Info] Trained a tree with leaves=31 and max_depth=8\n",
      "[LightGBM] [Info] Trained a tree with leaves=31 and max_depth=11\n",
      "[LightGBM] [Info] Trained a tree with leaves=31 and max_depth=11\n",
      "[LightGBM] [Info] Trained a tree with leaves=31 and max_depth=9\n",
      "[LightGBM] [Info] Trained a tree with leaves=31 and max_depth=9\n",
      "[LightGBM] [Info] Trained a tree with leaves=31 and max_depth=9\n",
      "[LightGBM] [Info] Trained a tree with leaves=31 and max_depth=9\n",
      "[LightGBM] [Info] Trained a tree with leaves=31 and max_depth=9\n",
      "[LightGBM] [Info] Trained a tree with leaves=31 and max_depth=9\n",
      "[LightGBM] [Info] Trained a tree with leaves=31 and max_depth=10\n",
      "08/06/2017 08:58:51 PM INFO: Cell returned\n",
      "08/06/2017 08:58:51 PM INFO: Running cell:\n",
      "clf.best_score_\n",
      "\n",
      "08/06/2017 08:58:51 PM INFO: Cell returned\n",
      "08/06/2017 08:58:51 PM INFO: Running cell:\n",
      "clf.best_params_\n",
      "\n",
      "08/06/2017 08:58:51 PM INFO: Cell returned\n",
      "08/06/2017 08:58:51 PM INFO: Running cell:\n",
      "# # clf.best_params_['n_estimators'] = \n",
      "clf.best_params_['max_depth'] = 6\n",
      "# clf.best_params_['reg_lambda'] = 0.5\n",
      "clf.best_params_['subsample'] = 0.8\n",
      "clf.best_params_['colsample_bytree'] = 0.8\n",
      "\n",
      "clf.best_params_['colsample_bylevel'] = 0.7\n",
      "\n",
      "# clf.best_params_['n_estimators'] = 1000\n",
      "clf.best_params_['learning_rate'] = 0.1\n",
      "clf.best_params_['num_leaves'] = 56\n",
      "\n",
      "\n",
      "# clf.best_params_['reg_alpha'] = 1\n",
      "# clf.best_params_['min_data_in_leaf'] = 3\n",
      "\n",
      "# clf.best_params_['min_child_weight'] = 1\n",
      "clf.best_params_['lambda_l2'] = 10\n",
      "clf.best_params_['tree_method'] = 'exact'\n",
      "clf.best_params_['metric'] = 'binary_logloss'\n",
      "\n",
      "\n",
      "\n",
      "08/06/2017 08:58:51 PM INFO: Cell returned\n",
      "08/06/2017 08:58:51 PM INFO: Running cell:\n",
      "2**6\n",
      "\n",
      "08/06/2017 08:58:51 PM INFO: Cell returned\n",
      "08/06/2017 08:58:51 PM INFO: Running cell:\n",
      "params = {\n",
      "    'boosting_type': 'gbdt',\n",
      "#     'objective': 'binary',\n",
      "#     'metric': 'binary_logloss',\n",
      "        'metric': 'auc',\n",
      "    'min_child_weight': 1.5,\n",
      "#     'num_leaves': 2 ** 7,\n",
      "    'max_depth':6\n",
      "      'num_leaves':32\n",
      "    'lambda_l2': 10,\n",
      "    'subsample': 0.7,\n",
      "    'colsample_bytree': 0.7,\n",
      "#     'colsample_bylevel': 0.7,\n",
      "    'learning_rate': 0.05,\n",
      "#     'tree_method': 'exact',\n",
      "}\n",
      "\n",
      "08/06/2017 08:58:51 PM INFO: Cell raised uncaught exception: \n",
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-16-ba32b451f1aa>\"\u001b[1;36m, line \u001b[1;32m9\u001b[0m\n",
      "\u001b[1;33m    'num_leaves':32\u001b[0m\n",
      "\u001b[1;37m               ^\u001b[0m\n",
      "\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n",
      "\n",
      "08/06/2017 08:58:51 PM INFO: Shutdown kernel\n",
      "08/06/2017 08:58:52 PM WARNING: Exiting with nonzero exit status\n"
     ]
    }
   ],
   "source": [
    "!runipy lgb_train.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "08/06/2017 10:08:51 PM INFO: Reading notebook lgb_predict_result.ipynb\n",
      "08/06/2017 10:08:51 PM INFO: Running cell:\n",
      "import pickle\n",
      "from datetime import datetime\n",
      "\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import scipy as sp\n",
      "\n",
      "# import matplotlib.pyplot as plt\n",
      "\n",
      "from sklearn import model_selection\n",
      "from sklearn.model_selection import KFold\n",
      "from sklearn.model_selection import train_test_split\n",
      "\n",
      "import xgboost as xgb\n",
      "from xgboost import XGBClassifier\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn import model_selection\n",
      "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
      "\n",
      "08/06/2017 10:08:53 PM INFO: Cell returned\n",
      "08/06/2017 10:08:53 PM INFO: Running cell:\n",
      "submit_path = datetime.now().strftime('../submits/%d-%H%M.csv')\n",
      "submit_path\n",
      "\n",
      "08/06/2017 10:08:53 PM INFO: Cell returned\n",
      "08/06/2017 10:08:53 PM INFO: Running cell:\n",
      "model = pickle.load(open('../dataset/lightgbmmodel.pkl', 'rb'))\n",
      "\n",
      "[LightGBM] [Info] Finished loading 1072 models\n",
      "08/06/2017 10:08:53 PM INFO: Cell returned\n",
      "08/06/2017 10:08:53 PM INFO: Running cell:\n",
      "test_feature = pd.read_pickle('../dataset/test/all_feature.pkl')\n",
      "test_feature.shape\n",
      "\n",
      "08/06/2017 10:08:56 PM INFO: Cell returned\n",
      "08/06/2017 10:08:56 PM INFO: Running cell:\n",
      "use_columns = [\n",
      "    x for x in test_feature.columns\n",
      "    if x not in [\n",
      "        'orderid', 'uid', 'hotelid', 'basicroomid', 'hotel_roomid', 'roomid', 'orderlabel'\n",
      "    ]\n",
      "]\n",
      "\n",
      "08/06/2017 10:08:56 PM INFO: Cell returned\n",
      "08/06/2017 10:08:56 PM INFO: Running cell:\n",
      "result = model.predict_proba(test_feature[use_columns])[:, 1]\n",
      "\n",
      "08/06/2017 10:09:44 PM INFO: Cell returned\n",
      "08/06/2017 10:09:44 PM INFO: Running cell:\n",
      "test_feature['prob'] = result\n",
      "\n",
      "08/06/2017 10:09:44 PM INFO: Cell returned\n",
      "08/06/2017 10:09:44 PM INFO: Running cell:\n",
      "test_feature.sort_values('prob', ascending=False, inplace=True)\n",
      "\n",
      "08/06/2017 10:09:46 PM INFO: Cell returned\n",
      "08/06/2017 10:09:46 PM INFO: Running cell:\n",
      "submit = test_feature.drop_duplicates(['orderid'])[['orderid', 'roomid', 'prob']]\n",
      "\n",
      "08/06/2017 10:09:47 PM INFO: Cell returned\n",
      "08/06/2017 10:09:47 PM INFO: Running cell:\n",
      "submit['orderid'] = 'ORDER_' + submit['orderid'].astype(str)\n",
      "\n",
      "08/06/2017 10:09:47 PM INFO: Cell returned\n",
      "08/06/2017 10:09:47 PM INFO: Running cell:\n",
      "submit['predict_roomid'] = 'ROOM_' + submit.roomid.astype(str)\n",
      "\n",
      "08/06/2017 10:09:47 PM INFO: Cell returned\n",
      "08/06/2017 10:09:47 PM INFO: Running cell:\n",
      "submit.to_csv(submit_path, index=False, columns=['orderid', 'predict_roomid'])\n",
      "\n",
      "08/06/2017 10:09:47 PM INFO: Cell returned\n",
      "08/06/2017 10:09:47 PM INFO: Running cell:\n",
      "!wc -l $submit_path\n",
      "\n",
      "08/06/2017 10:09:47 PM INFO: Cell returned\n",
      "08/06/2017 10:09:47 PM INFO: Running cell:\n",
      "!head $submit_path\n",
      "\n",
      "08/06/2017 10:09:47 PM INFO: Cell returned\n",
      "08/06/2017 10:09:47 PM INFO: Running cell:\n",
      "# test_feature.orderid.unique().shape\n",
      "\n",
      "08/06/2017 10:09:47 PM INFO: Cell returned\n",
      "08/06/2017 10:09:47 PM INFO: Shutdown kernel\n"
     ]
    }
   ],
   "source": [
    "!runipy lgb_predict_result.ipynb"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
